{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd1164d-8f00-4461-b099-67ee6cd35d9e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# ‚è±Ô∏è **job_scheduler_pattern**\n",
    "\n",
    "Sample implementation pattern for multiple scheduled executions in Microsoft Fabric.\n",
    "\n",
    "You can check more information on Medium:\n",
    "- [English version](https://medium.com/@baggirraf/%EF%B8%8F-one-to-schedule-them-all-scheduling-and-executing-processes-in-microsoft-fabric-ef50da361bc0)\n",
    "- [Spanish version](https://medium.com/@baggirraf/%EF%B8%8F-uno-para-programarlos-a-todos-agendado-y-ejecuci%C3%B3n-de-procesos-en-microsoft-fabric-775cfc36f720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e44998-04cc-4c29-a95c-088a92f6fac9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### **üß∑ Imports and references üß∑**\n",
    "\n",
    "Reference to the **_sempy_functions_** notebook available at [GitHub](https://github.com/l2aFa/rafabric/blob/main/pyspark/sempy_functions.ipynb).\n",
    "\n",
    "‚ö†Ô∏è This is only allowed for Spark notebooks. \n",
    "\n",
    "You will need to include the reference in the appropriate form or the functions themselves in the case of the Python notebook. \n",
    "\n",
    "Library imports and references will also be needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa06aad-633c-4a8a-b99e-fee8af744b96",
   "metadata": {
    "editable": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "%run sempy_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac721837-1c3b-4318-9046-dc3af8eae121",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### **üìö Use cases and examples üìö**\n",
    "\n",
    "Below are different examples of invoking different artefacts (pipeline, notebook, dataflow).\n",
    "\n",
    "Choose the one that best suits your scenario to implement it and then schedule the execution of the **_\"scheduler\"_** notebook.\n",
    "\n",
    "Please note:\n",
    "- The use of parameters is entirely optional. If your device does not have parameters, you can omit the payload variable declaration and its inclusion in the fabric_rest_api_caller function call.\n",
    "- In the specific case of Gen2 dataflows, it is necessary to enable the use of parameters that are in preliminary version, as indicated [here](https://learn.microsoft.com/en-us/fabric/data-factory/dataflow-parameters). Be aware of the limitations that this entails.\n",
    "- A list of allowed values for the artifact type can be found [here](https://learn.microsoft.com/en-us/rest/api/fabric/core/items/list-items?tabs=HTTP#itemtype).\n",
    "- However, it is important to point out that these values do not correspond to those allowed for the **_job_type_** value required in the API call. I have not found a list of valid values as in the previous case, but for the examples, this small mapping will suffice:\n",
    "\n",
    "```\n",
    "            artifact_type_to_job_type = {\n",
    "                \"DataPipeline\": \"Pipeline\", # Fabric Data Factory pipeline\n",
    "                \"Dataflow\": \"Refresh\", # Dataflow Gen2\n",
    "                \"Notebook\": \"RunNotebook\" # Spark/Python notebook\n",
    "                }\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76322a8-1328-4e1e-962c-44eb717b4c57",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### 1Ô∏è‚É£ Fabric Data Factory pipeline sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e83bfa-d1f4-4679-bf12-34bb4cfec593",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# We can grab the workspace identifier through sempy, specify its value or we can skip it and the later function will take the default value\n",
    "workspace_id = fabric.get_workspace_id()\n",
    "\n",
    "# We look for the identifier corresponding to the artefact we want to execute on a scheduled basis\n",
    "# We use the function get_artifact_guid_by_name to obtain its corresponding GUID, given that its name should not change between environments\n",
    "# As a demonstration, we make the call passing values for all arguments, including the optional ones.\n",
    "artifact_name = \"YOUR_PIPELINE_NAME_HERE\"\n",
    "artifact_type = \"DataPipeline\"\n",
    "artifact_id = get_artifact_guid_by_name(artifact_name= artifact_name, workspace= workspace_id, artifact_type=artifact_type)\n",
    "if not artifact_id:\n",
    "    raise Exception(f\"*ERROR*: ‚ùå Artifact '{artifact_name}' not found in workspace '{workspace_id}.'\")\n",
    "\n",
    "# Parameters of the invoked artefact\n",
    "# Can be skipped if artifact has no parameters\n",
    "scheduler_sample_param_01 = \"I was passed first\"\n",
    "scheduler_sample_param_02 = \"I was second\"\n",
    "\n",
    "# Prepare arguments for the call\n",
    "job_type = \"Pipeline\"\n",
    "rest_api_uri = f\"v1/workspaces/{workspace_id}/items/{artifact_id}/jobs/instances?jobType={job_type}\"\n",
    "# The arguments for the call can also be skipped if the artifact has no parameters as well\n",
    "# The values enclosed in quotation marks within the parameters section must have the same value as the parameter names defined in the pipeline.\n",
    "payload = {\n",
    "    \"executionData\": {\n",
    "        \"parameters\": {\n",
    "            \"pipeline_sample_param_01\": scheduler_sample_param_01,\n",
    "            \"pipeline_sample_param_02\": scheduler_sample_param_02\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Request artifact execution\n",
    "# As a demonstration, we make the call passing values for all arguments, including the optional ones.\n",
    "response = fabric_rest_api_caller(source_uri=rest_api_uri, method=\"post\", audience=\"fabric\", source_payload=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a0279-dd17-4300-8fbb-342ed3f6c6f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### 2Ô∏è‚É£ Spark/Python notebook sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33e7e95-26ff-4689-bd92-089e6207cd14",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# This time we specify the workspace guid/name where the artifact lives\n",
    "# We do not specify artifact_type\n",
    "workspace_id = \"YOUR_WORKSPACE_GUID/NAME_HERE\"\n",
    "artifact_name = \"YOUR_NOTEBOOK_NAME_HERE\"\n",
    "artifact_id = get_artifact_guid_by_name(artifact_name)\n",
    "if not artifact_id:\n",
    "    raise Exception(f\"*ERROR*: ‚ùå Artifact '{artifact_name}' not found in workspace '{workspace_id}.'\")\n",
    "\n",
    "scheduler_sample_param_01 = \"I was passed first\"\n",
    "scheduler_sample_param_02 = \"I was second\"\n",
    "\n",
    "job_type = \"RunNotebook\"\n",
    "rest_api_uri = f\"v1/workspaces/{workspace_id}/items/{artifact_id}/jobs/instances?jobType={job_type}\"\n",
    "# The values enclosed in quotation marks within the parameters section must have the same value as the parameter names defined in the notebook parameter cell.\n",
    "payload = {\n",
    "  \"executionData\": {\n",
    "    \"parameters\": {\n",
    "      \"notebook_sample_param_01\": {\n",
    "        \"value\": scheduler_sample_param_01,\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"notebook_sample_param_02\": {\n",
    "        \"value\": scheduler_sample_param_01,\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "response = fabric_rest_api_caller(source_uri=rest_api_uri, method=\"post\", audience=\"fabric\", source_payload=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235eff20-4152-4bdd-b1d2-63f38cc11d89",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### 3Ô∏è‚É£ Dataflow Gen2 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ffac3-d9f7-48f4-83fb-2a00a7ee31af",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# No workspace specified for the artifact\n",
    "# artifact_type specified\n",
    "artifact_name = \"YOUR_DATAFLOW_NAME_HERE\"\n",
    "artifact_type = \"Dataflow\"\n",
    "artifact_id = get_artifact_guid_by_name(artifact_name= artifact_name, artifact_type=artifact_type)\n",
    "if not artifact_id:\n",
    "    raise Exception(f\"*ERROR*: ‚ùå Artifact '{artifact_name}' not found.'\")\n",
    "\n",
    "scheduler_sample_param_01 = \"I was passed first\"\n",
    "scheduler_sample_param_02 = \"I was second\"\n",
    "\n",
    "job_type = \"Refresh\"\n",
    "rest_api_uri = f\"v1/workspaces/{workspace_id}/items/{artifact_id}/jobs/instances?jobType={job_type}\"\n",
    "# The values enclosed in quotation marks for each parameterName attribute must have the same value and type as the parameter names defined in the dataflow parameters\n",
    "# type attribute values are the same as the ones available for PowerQuery parameters\n",
    "# Remember to enable the parameters check in the dataflow options\n",
    "payload = {\n",
    "  \"executionData\": {\n",
    "    \"parameters\": [\n",
    "      {\n",
    "        \"parameterName\": \"dataflow_sample_param_01\",\n",
    "        \"type\": \"Text\",\n",
    "        \"value\": scheduler_sample_param_01\n",
    "      },\n",
    "      {\n",
    "        \"parameterName\": \"dataflow_sample_param_02\",\n",
    "        \"type\": \"Text\",\n",
    "        \"value\": scheduler_sample_param_02\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "response = fabric_rest_api_caller(source_uri=rest_api_uri, method=\"post\", audience=\"fabric\", source_payload=payload)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "300000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
